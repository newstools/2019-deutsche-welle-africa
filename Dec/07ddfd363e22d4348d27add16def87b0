*NB: This interview was subjected to an approval process by the Federal Ministry for Education and Research (BMBF) before we were allowed to publish it. The text has been redacted and altered by the BMBF in addition to DW's normal editorial guidelines. As such, the text does not entirely reflect the audio of the interview as recorded on December 5, 2019. DW: We're in Berlin at an "Artificial Intelligence Camp" organized by the Gesellschaft für Informatik and the German Federal Ministry of Education and Research, where you head the department for "Research for Digitalization and Innovation." Artificial intelligence is in your remit. And all the people here are experts in the field. But AI is one of those topics where lots of people talk about it, but few people really understand it, especially in the wider public. So what use is an event like this? Prof. Dr.-Ing. (Doctor of Engineering) Ina Schieferdecker: We must always consider AI in its social context. That means the debate about AI must include interdisciplinary perspectives, so that scientists from different disciplines can tackle the challenges together. Read more: German-engineered emotion-sensing robot heads to space We hope that events like this will inspire creative ideas that can then be implemented, whether that's in health, transport or energy. We also hope that those new ideas are communicated beyond events such as this and our "Science Year for Artificial Intelligence" so that the public can engage with them. Ina Schieferdecker speaking at KI-Camp ("AI Camp") in Berlin, December 5, 2019. There are, however, lots of challenges with artificial intelligence, not least a certain amount of fear among non-experts. You get the feeling sometimes that AI is a controversial topic. But how controversial is it, do you think? Are there aspects of AI that are best left out of the debate? Or aspects that we simply can't discuss because we don't fully know where this technology is going? I think we should discuss all aspects of AI, including people's fears and concerns. We must find ways to do that using our scientific expertise to address the questions, without disregarding emotional responses to the topic. And we should remember that technology has to be for the common good. But for that to work it will take a huge effort on everyone's part. Listen to the New Peers Review tackling AI on DW's science weekly show, Spectrum Send  Facebook   Twitter   google+   Whatsapp   Tumblr   linkedin   stumble   Digg   reddit   Newsvine Permalink https://p.dw.com/p/3UYpE Discussion is good. But when will we see tangible decisions about AI, and enforceable laws? First, it has to be said that we're already seeing AI in the public sphere. It's used in search engines, on social media, or to optimize transport, mobility, and there are guidelines to help us build AIs according to our basic principles. But the process is ongoing. We have to ensure that all actors in the field understand the potential benefits and risks, and which options and responsibilities we all have. So in terms of decisions, it's not as if we can decide something today and that tomorrow it will all be different. Rather, it's like I said, a huge, communal effort. We all have to think consciously about AI and take action, whether that's in science, education or business. We must come together and can create a European-style AI. People often describe AI as a "black box," as if to say, We don't know what's happening, or how AIs make decisions. Personally, I think humans are among the biggest black boxes, especially on this very topic — humans often act or make split-second decisions that we don't truly understand. But I'd still say we need a tangible vision. I see things differently. I'm often on panel discussions and hear people say that algorithms should be made transparent. And then I come along with my university degree, which has enabled me to develop and understand algorithms... But I don't believe that everyone has to understand AI. Not everyone can understand it. Technology should be trustworthy. But we don't all understand how planes work or how giant tankers float on water. So, we have learn to trust digital technology, too. Send  Facebook   Twitter   google+   Whatsapp   Tumblr   linkedin   stumble   Digg   reddit   Newsvine Permalink https://p.dw.com/p/3RyaO That's what we have to demand from AI developers, that they consider things like people's safety concerns and our other basic principles from the very start. We also need processes and tools to hand so that if something does go wrong — and every technical system has its limitations, so there will be unexpected decisions and recommendations by AIs — tools that help us understand those decisions and, if necessary, counter decisions where they affect people or organizations. Zulfikar Abbany (front) recorded an episode of the New Peers Review, a co-production with the University of Strathclyde's Malcolm Macdonald (left) and Ciara McGrath (right) Well, that is precisely what we want to discuss in our program, the New Peers Review, which we're recording here — how much does the public need to know in order to make informed decisions? If you look at the European Union's recommendations on a "Trustworthy AI," there's a lot of room for interpretation. The EU says AI should be human-centric, legal, ethical, robust. So when we then look at military uses of AI, there's a lot there that's technically legal but which wouldn't be legal in other areas of life. It makes it very hard for the public to make responsible decisions and for the public to know what it is voting for in its politicians. That is a relevant aspect, but I can only speak on civilian issues. I don't want to speak on military aspects, nor do I want to express my opinion. Send  Facebook   Twitter   google+   Whatsapp   Tumblr   linkedin   stumble   Digg   reddit   Newsvine Permalink https://p.dw.com/p/3RYcN No, I'm not asking you to do that. I'm simply trying to illustrate that, while we love to talk up positive aspects of AI, we seldom talk about the negative aspects. And even in commerce, whether you're buying a book or streaming a film, few people know what's happening in the background… Yes, okay, but that's more to do with personal data. And we're trying to achieve a form of "data sovereignty" and offer people alternatives to your Amazons, Googles, Instagrams… And I have to say that Europe's data protection regulations have made a difference. Sure, the first version of that has had its issues, here and there, but it's being further developed. Read more: UN impasse could mean killer robots escape regulation The EU's Trustworthy AI feels like it was driven heavily by Germany. How well placed is Germany to make Europe's AI future look the way this country wants it? Send  Facebook   Twitter   google+   Whatsapp   Tumblr   linkedin   stumble   Digg   reddit   Newsvine Permalink https://p.dw.com/p/3U9NS First, we're very strong in science and we've managed to get more and more science into technology. Second, our initial steps on regulation have been in the right direction. And third, we're soon to hold the EU presidency (July-December 2020) and when we do we will try to pool Europe's strengths in an effort to transform the rules on paper into something real and useable for the people. Professor Ina Schieferdecker is director of "Research for Digitalization and Innovation" at the German Federal Ministry of Education and Research. Schieferdecker is a computer scientist who has held positions at the TU Berlin (Technische Universität Berlin) and the Fraunhofer Institute for Open Communication Systems. She is a founding director of the Weizenbaum Institute for the Networked Society. And she was speaking at "KI-Camp," an event organized by the Gesellschaft für Informatik and the BMBF's Science Year initiative. The New Peers Review is a co-production by DW and the University of Strathclyde.